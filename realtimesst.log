2025-02-22 20:43:22.447 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:43:22.460 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:43:22.466 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:43:22.467 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:43:25.813 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:43:26.116 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:43:26.120 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:43:26.425 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:43:26.426 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:43:26.736 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:43:26.738 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:43:26.738 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:43:27.169 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:43:27.169 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:43:27.169 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:43:29.043 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:43:29.043 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:43:29.043 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:43:29.043 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'recording'
2025-02-22 20:43:29.082 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:43:43.500 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:43:43.504 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:43:43.508 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 223232
2025-02-22 20:43:43.508 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:43:43.509 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:43:43.513 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:43.617 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:43.719 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:43.849 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:43.952 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:44.053 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:44.154 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:43:44.199 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:43:44.241 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.73 seconds
2025-02-22 20:44:49.587 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:44:49.599 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:44:49.605 - RealTimeSTT: root - ERROR - Error initializing porcupine wake word detection engine: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib, 0x0006): tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (no such file), '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py", line 769, in __init__
    self.porcupine = pvporcupine.create(
                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/__init__.py", line 64, in create
    return Porcupine(
           ^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/porcupine.py", line 60, in __init__
    library = cdll.LoadLibrary(library_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 460, in LoadLibrary
    return self._dlltype(name)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib, 0x0006): tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (no such file), '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))
2025-02-22 20:45:00.642 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:45:00.651 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:45:00.656 - RealTimeSTT: root - ERROR - Error initializing porcupine wake word detection engine: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib, 0x0006): tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (no such file), '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py", line 769, in __init__
    self.porcupine = pvporcupine.create(
                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/__init__.py", line 64, in create
    return Porcupine(
           ^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/porcupine.py", line 60, in __init__
    library = cdll.LoadLibrary(library_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 460, in LoadLibrary
    return self._dlltype(name)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib, 0x0006): tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (no such file), '/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/pvporcupine/lib/mac/x86_64/libpv_porcupine.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e' or 'arm64'))
2025-02-22 20:45:16.191 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:45:16.201 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:45:16.206 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:45:16.207 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:45:16.690 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:45:16.692 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:45:16.694 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:45:16.695 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:45:16.695 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:45:16.696 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:45:16.696 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:45:16.696 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:45:16.781 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:45:16.781 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:45:16.781 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:45:19.992 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:45:19.992 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:45:19.992 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:19.992 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:20.217 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:21.884 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:21.884 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:21.884 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:21.884 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:24.251 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:24.253 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 54272
2025-02-22 20:45:24.254 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:24.344 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:24.346 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:24.371 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.472 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.573 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.674 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.775 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.876 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:24.958 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:24.980 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.63 seconds
2025-02-22 20:45:24.981 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:24.981 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:24.981 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:25.147 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:25.147 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:25.147 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:25.147 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:26.300 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:26.301 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:45:26.301 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:26.369 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:26.371 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:26.388 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.490 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.591 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.691 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.793 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.894 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:26.995 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:27.037 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:27.044 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.67 seconds
2025-02-22 20:45:27.045 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:27.045 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:27.046 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:30.143 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:30.144 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:30.144 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:30.144 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:31.551 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:31.552 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 38912
2025-02-22 20:45:31.553 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:31.616 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:31.617 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:31.620 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:31.721 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:31.823 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:31.924 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:32.025 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:32.126 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:32.183 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:32.186 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.57 seconds
2025-02-22 20:45:32.187 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:32.187 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:32.187 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:34.621 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:34.621 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:34.621 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:34.622 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:37.051 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:37.053 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 55296
2025-02-22 20:45:37.053 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:37.104 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:37.105 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:37.107 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.208 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.309 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.410 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.511 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.613 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.714 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:37.814 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:37.855 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.75 seconds
2025-02-22 20:45:37.855 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:37.855 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:37.856 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:39.227 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:39.227 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:39.227 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:39.227 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:40.766 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:40.768 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 40960
2025-02-22 20:45:40.768 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:40.805 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:40.806 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:40.827 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:40.929 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.033 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.134 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.235 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.337 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.437 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:41.479 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:41.521 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.71 seconds
2025-02-22 20:45:41.521 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:41.521 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:41.521 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:44.223 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:44.223 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:44.223 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:44.223 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:45.373 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:45.374 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:45:45.374 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:45.455 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:45.456 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:45.479 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:45.580 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:45.682 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:45.782 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:45.883 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:45.985 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:46.086 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:46.086 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:46.108 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.65 seconds
2025-02-22 20:45:46.108 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:46.108 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:46.108 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:48.061 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:48.062 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:48.062 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:48.062 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:49.216 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:49.217 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:45:49.218 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:49.264 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:49.266 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:49.285 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.394 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.496 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.597 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.699 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.800 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:49.879 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:49.888 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.62 seconds
2025-02-22 20:45:49.888 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:49.889 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:49.889 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:45:52.091 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:45:52.091 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:45:52.091 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:45:52.092 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:45:54.909 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:45:54.910 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 61440
2025-02-22 20:45:54.910 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:45:54.965 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:45:54.965 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:45:54.972 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.073 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.174 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.276 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.377 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.478 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.579 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.680 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:45:55.739 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:45:55.742 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.78 seconds
2025-02-22 20:45:55.743 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:45:55.743 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:45:55.743 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:46:02.081 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:46:02.081 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:46:02.081 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:46:02.082 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:46:05.146 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:46:05.147 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 65536
2025-02-22 20:46:05.147 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:46:05.198 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:46:05.199 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:46:05.215 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.319 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.421 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.522 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.623 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.724 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.825 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:05.902 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:46:05.928 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.73 seconds
2025-02-22 20:46:05.928 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:46:05.928 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:46:05.928 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:46:06.046 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:46:06.046 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:46:06.046 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:46:06.046 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:46:07.200 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:46:07.202 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:46:07.202 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:46:07.225 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:46:07.227 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:46:07.229 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.329 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.431 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.532 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.633 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.734 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:07.835 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:46:07.883 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.66 seconds
2025-02-22 20:46:07.883 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:46:07.883 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:46:07.883 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:46:11.229 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:46:11.230 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:46:11.230 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:46:11.230 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:46:12.765 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:46:12.767 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 40960
2025-02-22 20:46:12.767 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:46:12.782 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:46:12.784 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:46:12.792 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:12.897 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:12.998 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.104 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.205 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.307 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.408 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.509 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:46:13.511 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:46:13.542 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.76 seconds
2025-02-22 20:46:13.542 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:46:13.542 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:46:13.542 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:46:16.819 - RealTimeSTT: root - INFO - KeyboardInterrupt in wait_audio, shutting down
2025-02-22 20:46:16.820 - RealTimeSTT: root - DEBUG - Finishing recording thread
2025-02-22 20:46:16.831 - RealTimeSTT: root - DEBUG - Terminating reader process
2025-02-22 20:46:17.431 - RealTimeSTT: root - DEBUG - Terminating transcription process
2025-02-22 20:46:17.432 - RealTimeSTT: root - DEBUG - Finishing realtime thread
2025-02-22 20:46:17.510 - RealTimeSTT: root - INFO - KeyboardInterrupt in text() method
2025-02-22 20:47:05.097 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:47:05.107 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:47:05.111 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:47:05.111 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:47:05.578 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:47:05.579 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:47:05.581 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:47:05.582 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:47:05.582 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:47:05.583 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:47:05.583 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:47:05.583 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:47:05.665 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:47:05.665 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:47:05.665 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:47:08.715 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:47:08.715 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:47:08.715 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:08.715 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:08.946 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:47:10.328 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:47:10.328 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:47:10.328 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:47:10.328 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:47:14.677 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:47:14.680 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 86016
2025-02-22 20:47:14.680 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:47:14.718 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:47:14.720 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:47:14.732 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:14.835 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:14.937 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.038 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.139 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.241 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.342 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.443 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:15.474 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:47:15.530 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.81 seconds
2025-02-22 20:47:15.530 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:15.530 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:15.530 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:47:20.948 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:47:20.949 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:47:20.949 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:47:20.949 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:47:24.341 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:47:24.343 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 70656
2025-02-22 20:47:24.343 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:47:24.434 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:47:24.436 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:47:24.459 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:24.560 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:24.661 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:24.762 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:24.864 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:24.964 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.066 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.170 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.271 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.372 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.473 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.575 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.676 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.777 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.878 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:25.980 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:26.027 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:47:26.030 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.59 seconds
2025-02-22 20:47:26.030 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:26.030 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:26.031 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:47:33.046 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:47:33.047 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:47:33.047 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:47:33.048 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:47:36.500 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:47:36.502 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 71680
2025-02-22 20:47:36.503 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:47:36.532 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:47:36.533 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:47:36.556 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:36.661 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:36.761 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:36.864 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:36.965 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:37.066 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:37.168 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:37.269 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:37.348 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:47:37.371 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.84 seconds
2025-02-22 20:47:37.372 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:37.372 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:37.372 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:47:45.013 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:47:45.013 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:47:45.013 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:47:45.013 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:47:47.510 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:47:47.512 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 56320
2025-02-22 20:47:47.512 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:47:47.526 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:47:47.528 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:47:47.537 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:47.654 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:47.755 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:47.857 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:47.957 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:48.058 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:48.159 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:48.168 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:47:48.197 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.67 seconds
2025-02-22 20:47:48.197 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:48.197 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:48.198 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:47:50.258 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:47:50.259 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:47:50.259 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:47:50.259 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:47:52.563 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:47:52.566 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 53248
2025-02-22 20:47:52.566 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:47:52.657 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:47:52.658 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:47:52.666 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:52.768 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:52.869 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:52.971 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.072 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.174 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.275 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.376 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.477 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.579 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:47:53.669 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:47:53.719 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.06 seconds
2025-02-22 20:47:53.719 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:47:53.719 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:47:53.720 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:02.931 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:02.932 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:02.932 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:02.933 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:05.554 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:05.556 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 58368
2025-02-22 20:48:05.556 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:05.586 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:05.587 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:05.606 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:05.708 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:05.810 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:05.910 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:06.012 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:06.113 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:06.214 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:06.316 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:06.351 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:06.391 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.80 seconds
2025-02-22 20:48:06.391 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:06.391 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:06.392 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:12.278 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:12.278 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:12.278 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:12.278 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:14.642 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:14.644 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 54272
2025-02-22 20:48:14.645 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:14.668 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:14.669 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:14.687 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:14.788 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:14.960 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:15.064 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:15.165 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:15.267 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:15.298 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:15.335 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.67 seconds
2025-02-22 20:48:15.335 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:15.335 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:15.335 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:17.586 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:17.587 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:17.588 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:17.589 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:19.061 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:19.063 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 39936
2025-02-22 20:48:19.064 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:19.146 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:19.148 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:19.150 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.251 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.352 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.453 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.555 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.656 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:19.753 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:19.794 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.65 seconds
2025-02-22 20:48:19.794 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:19.794 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:19.795 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:22.068 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:22.068 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:22.068 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:22.069 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:23.218 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:23.220 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:48:23.220 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:23.316 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:23.318 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:23.320 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.421 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.523 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.624 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.725 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.826 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:23.927 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:24.020 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:24.034 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.72 seconds
2025-02-22 20:48:24.034 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:24.034 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:24.034 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:28.982 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:28.982 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:28.982 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:28.983 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:30.835 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:30.837 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 46080
2025-02-22 20:48:30.837 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:30.840 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:30.841 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:30.859 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:30.964 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.065 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.165 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.267 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.368 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.470 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.571 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.672 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:31.672 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:31.682 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.84 seconds
2025-02-22 20:48:31.682 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:31.682 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:31.683 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:33.846 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:33.846 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:33.846 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:33.846 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:35.764 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:35.766 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 47104
2025-02-22 20:48:35.766 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:35.849 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:35.849 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:35.858 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:35.960 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.061 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.161 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.262 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.363 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.465 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.566 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:36.654 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:36.686 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.84 seconds
2025-02-22 20:48:36.686 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:36.686 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:36.687 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:39.349 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:39.349 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:39.349 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:39.350 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:41.462 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:41.464 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 50176
2025-02-22 20:48:41.464 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:41.563 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:41.565 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:41.571 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:41.672 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:41.775 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:41.876 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:41.979 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:42.081 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:42.182 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:42.283 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:42.384 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:42.419 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:42.449 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.88 seconds
2025-02-22 20:48:42.449 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:42.449 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:42.449 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:45.810 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:45.810 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:45.810 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:45.811 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:47.731 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:47.732 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 47104
2025-02-22 20:48:47.733 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:47.774 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:47.775 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:47.799 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:47.900 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.002 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.103 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.204 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.319 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.421 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.522 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.623 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.724 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:48.775 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:48.800 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.02 seconds
2025-02-22 20:48:48.800 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:48.800 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:48.800 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:55.156 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:55.157 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:55.157 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:55.157 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:48:56.759 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:48:56.760 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 41984
2025-02-22 20:48:56.761 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:48:56.763 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:48:56.764 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:48:56.766 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:56.870 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:56.972 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.072 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.183 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.284 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.385 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.487 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.588 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:48:57.588 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:48:57.621 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.86 seconds
2025-02-22 20:48:57.621 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:48:57.621 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:48:57.621 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:48:59.765 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:48:59.765 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:48:59.765 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:48:59.766 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:01.298 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:01.300 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 40960
2025-02-22 20:49:01.300 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:01.331 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:01.332 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:01.339 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.440 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.540 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.642 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.743 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.844 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:01.920 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:01.943 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.61 seconds
2025-02-22 20:49:01.943 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:01.943 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:01.943 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:06.355 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:06.356 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:06.356 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:06.356 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:07.510 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:07.511 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:49:07.511 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:07.531 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:07.532 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:07.540 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:07.648 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:07.837 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:07.939 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.039 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.141 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.242 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.343 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.495 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.596 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.698 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.800 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:08.925 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:09.014 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:09.032 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.50 seconds
2025-02-22 20:49:09.032 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:09.032 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:09.033 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:14.487 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:14.487 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:14.488 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:14.488 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:17.045 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:17.047 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 57344
2025-02-22 20:49:17.047 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:17.090 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:17.091 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:17.098 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.199 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.299 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.400 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.501 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.603 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.704 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.805 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:17.906 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.007 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.109 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.210 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.311 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.412 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:18.497 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:18.510 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.42 seconds
2025-02-22 20:49:18.511 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:18.511 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:18.511 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:18.709 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:18.710 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:18.710 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:18.710 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:27.411 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:27.414 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 155648
2025-02-22 20:49:27.417 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:27.513 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:27.515 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:27.531 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:27.638 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:27.744 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:27.846 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.000 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.125 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.260 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.361 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.463 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.576 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.678 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.783 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:28.887 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.013 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.123 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.233 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.337 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.438 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.540 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.641 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.743 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.847 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:29.949 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.050 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.151 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.252 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.354 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.455 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:30.548 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:30.590 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 3.07 seconds
2025-02-22 20:49:30.591 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:30.591 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:30.592 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:32.472 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:32.472 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:32.472 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:32.472 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:34.071 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:34.072 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 41984
2025-02-22 20:49:34.073 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:34.082 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:34.083 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:34.097 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.198 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.299 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.399 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.500 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.606 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.707 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.809 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:34.910 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:35.011 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:35.025 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:35.052 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.97 seconds
2025-02-22 20:49:35.052 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:35.052 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:35.052 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:35.219 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:35.219 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:35.219 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:35.219 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:40.212 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:40.215 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 96256
2025-02-22 20:49:40.215 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:40.277 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:40.278 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:40.284 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.387 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.491 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.593 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.694 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.795 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.897 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:40.998 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.140 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.259 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.360 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.462 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.563 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.765 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.870 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:41.938 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:41.980 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.70 seconds
2025-02-22 20:49:41.980 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:41.980 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:41.980 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:43.029 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:43.029 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:43.029 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:43.029 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:44.101 - RealTimeSTT: root - INFO - KeyboardInterrupt in wait_audio, shutting down
2025-02-22 20:49:44.102 - RealTimeSTT: root - DEBUG - Finishing recording thread
2025-02-22 20:49:44.110 - RealTimeSTT: root - DEBUG - Terminating reader process
2025-02-22 20:49:44.669 - RealTimeSTT: root - DEBUG - Terminating transcription process
2025-02-22 20:49:44.670 - RealTimeSTT: root - DEBUG - Finishing realtime thread
2025-02-22 20:49:44.751 - RealTimeSTT: root - INFO - KeyboardInterrupt in text() method
2025-02-22 20:49:51.726 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:49:51.744 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:49:51.749 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:49:51.750 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:49:52.212 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:49:52.213 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:49:52.215 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:49:52.216 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:49:52.216 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:49:52.217 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:49:52.217 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:49:52.217 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:49:52.308 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:49:52.309 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:49:52.309 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:49:55.414 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:49:55.414 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:49:55.414 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:55.414 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:55.627 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:49:57.250 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:49:57.250 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:49:57.250 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:49:57.250 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:49:58.914 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:49:58.915 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 43008
2025-02-22 20:49:58.915 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:49:58.952 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:49:58.953 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:49:58.955 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.062 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.164 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.265 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.366 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.467 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.568 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.669 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:49:59.689 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:49:59.711 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.76 seconds
2025-02-22 20:49:59.712 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:49:59.712 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:49:59.712 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:03.908 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:03.909 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:03.909 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:03.909 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:05.893 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:05.895 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 48128
2025-02-22 20:50:05.895 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:05.979 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:05.980 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:06.004 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.105 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.207 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.309 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.414 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.516 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.616 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.718 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:06.808 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:06.827 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.85 seconds
2025-02-22 20:50:06.827 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:06.827 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:06.828 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:07.940 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:07.940 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:07.940 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:07.940 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:09.284 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:09.285 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 37888
2025-02-22 20:50:09.286 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:09.325 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:09.326 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:09.330 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.435 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.539 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.640 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.741 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.842 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:09.903 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:09.955 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.63 seconds
2025-02-22 20:50:09.955 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:09.955 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:09.955 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:10.688 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:10.688 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:10.689 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:10.689 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:12.483 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:12.485 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 45056
2025-02-22 20:50:12.485 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:12.556 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:12.557 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:12.559 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:12.670 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:12.771 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:12.873 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:12.974 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:13.075 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:13.177 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:13.196 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:13.244 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.69 seconds
2025-02-22 20:50:13.244 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:13.244 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:13.245 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:13.313 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:13.313 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:13.313 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:13.313 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:18.949 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:18.952 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 106496
2025-02-22 20:50:18.952 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:18.996 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:18.997 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:19.006 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.107 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.208 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.309 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.410 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.511 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.612 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.714 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.815 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:19.916 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.017 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.118 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.219 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.321 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.421 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.555 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:20.564 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:20.612 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.61 seconds
2025-02-22 20:50:20.612 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:20.612 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:20.612 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:20.737 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:20.737 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:20.737 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:20.737 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:21.893 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:21.895 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:50:21.895 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:21.988 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:21.990 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:22.007 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.114 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.216 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.317 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.418 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.519 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:22.564 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:22.581 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.59 seconds
2025-02-22 20:50:22.581 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:22.581 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:22.581 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:22.657 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:22.657 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:22.657 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:22.657 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:23.810 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:23.810 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:50:23.810 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:23.869 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:23.869 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:23.887 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:23.989 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:24.091 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:24.192 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:24.294 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:24.395 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:24.477 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:24.495 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.63 seconds
2025-02-22 20:50:24.495 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:24.495 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:24.495 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:26.114 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:26.114 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:26.114 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:26.115 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:30.532 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:30.535 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 87040
2025-02-22 20:50:30.535 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:30.536 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:30.537 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:30.542 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:30.644 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:30.744 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:30.845 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:30.947 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.048 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.149 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.250 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.351 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.453 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.554 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.655 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.756 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.857 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:31.958 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.060 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.160 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.261 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.362 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.464 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.566 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.667 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.768 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.869 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:32.970 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.071 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.172 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.274 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.375 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.476 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.577 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.678 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.779 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.884 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:33.991 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.093 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.194 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.295 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.396 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.497 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.599 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.700 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.801 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:34.902 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:35.004 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:35.087 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:35.094 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 4.56 seconds
2025-02-22 20:50:35.094 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:35.094 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:35.095 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:36.226 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:36.227 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:36.227 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:36.227 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:44.869 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:44.873 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 154624
2025-02-22 20:50:44.873 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:44.903 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:44.905 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:44.913 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.018 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.119 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.223 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.324 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.425 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.525 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.627 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.728 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:45.833 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:46.137 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:46.241 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:46.342 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:46.444 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:46.493 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:46.514 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.61 seconds
2025-02-22 20:50:46.514 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:46.514 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:46.514 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:46.595 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:46.595 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:46.595 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:46.595 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:53.058 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:53.062 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 119808
2025-02-22 20:50:53.062 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:53.088 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:53.090 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:53.096 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.201 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.302 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.403 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.505 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.606 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.707 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.809 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:53.910 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:54.011 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:54.112 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:54.119 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:54.147 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.06 seconds
2025-02-22 20:50:54.148 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:54.148 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:54.148 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:54.849 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:54.849 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:54.850 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:54.850 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:50:56.900 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:50:56.901 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 49152
2025-02-22 20:50:56.901 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:50:57.000 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:50:57.002 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:50:57.026 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.129 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.236 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.338 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.439 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.539 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.641 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.742 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.843 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:50:57.940 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:50:57.958 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.96 seconds
2025-02-22 20:50:57.958 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:50:57.958 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:50:57.959 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:50:58.053 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:50:58.053 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:50:58.053 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:50:58.053 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:51:00.101 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:51:00.104 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 49152
2025-02-22 20:51:00.104 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:51:00.185 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:51:00.186 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:51:00.206 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.307 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.408 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.510 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.611 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.712 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.814 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:00.915 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:01.016 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:01.117 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:01.218 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:01.231 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:51:01.242 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.06 seconds
2025-02-22 20:51:01.243 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:51:01.243 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:51:01.243 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:51:01.441 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:51:01.441 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:51:01.441 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:51:01.441 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:51:09.249 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:51:09.252 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 141312
2025-02-22 20:51:09.253 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:51:09.270 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:51:09.271 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:51:09.284 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.391 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.492 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.593 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.696 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.798 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:09.900 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.001 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.102 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.203 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.305 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.406 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.507 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:10.513 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:51:10.544 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.27 seconds
2025-02-22 20:51:10.544 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:51:10.544 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:51:10.545 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:51:10.656 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:51:10.656 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:51:10.656 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:51:10.656 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:51:12.769 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:51:12.771 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 50176
2025-02-22 20:51:12.774 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:51:12.873 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:51:12.874 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:51:12.880 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:12.981 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.092 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.193 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.294 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.395 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.497 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:13.559 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:51:13.581 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.71 seconds
2025-02-22 20:51:13.583 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:51:13.583 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:51:13.583 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:51:13.664 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:51:13.665 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:51:13.665 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:51:13.665 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:51:16.225 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:51:16.227 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 57344
2025-02-22 20:51:16.227 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:51:16.230 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:51:16.230 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:51:16.233 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.334 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.435 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.537 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.637 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.738 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.839 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:16.940 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.041 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.142 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.243 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.345 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.446 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.547 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:51:17.596 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:51:17.617 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 1.39 seconds
2025-02-22 20:51:17.617 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:51:17.617 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:51:17.618 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:51:32.996 - RealTimeSTT: root - INFO - KeyboardInterrupt in wait_audio, shutting down
2025-02-22 20:51:32.997 - RealTimeSTT: root - DEBUG - Finishing recording thread
2025-02-22 20:51:33.005 - RealTimeSTT: root - DEBUG - Terminating reader process
2025-02-22 20:51:33.597 - RealTimeSTT: root - DEBUG - Terminating transcription process
2025-02-22 20:51:33.598 - RealTimeSTT: root - DEBUG - Finishing realtime thread
2025-02-22 20:51:33.681 - RealTimeSTT: root - INFO - KeyboardInterrupt in text() method
2025-02-22 20:56:11.179 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:56:11.193 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:56:11.203 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:56:11.204 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:56:11.706 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:56:11.707 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:56:11.709 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:56:11.710 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:56:11.710 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:56:11.711 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:56:11.712 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:56:11.712 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:56:11.841 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:56:11.841 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:56:11.841 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:56:14.998 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:56:14.998 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:56:14.999 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:56:14.999 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:56:15.229 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:56:16.609 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:56:16.609 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:56:16.609 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:56:16.609 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:56:22.047 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:56:22.053 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 103424
2025-02-22 20:56:22.054 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:56:22.142 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:56:22.143 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:56:22.154 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:56:22.198 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:56:22.225 - RealTimeSTT: root - ERROR - Transcription error: 'english' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
2025-02-22 20:56:22.226 - RealTimeSTT: root - ERROR - Error during transcription: 'english' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py", line 1464, in transcribe
    raise Exception(result)
Exception: 'english' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
2025-02-22 20:57:39.651 - RealTimeSTT: root - DEBUG - Receive from stdout pipe
2025-02-22 20:57:48.280 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:57:48.292 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:57:48.296 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:57:48.297 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:57:48.761 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:57:48.763 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:57:48.766 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:57:48.767 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:57:48.767 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:57:48.768 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:57:48.768 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:57:48.768 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:57:48.888 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:57:48.888 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:57:48.888 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:57:52.382 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:57:52.382 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:57:52.382 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:57:52.382 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:57:52.590 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:57:52.768 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:57:52.768 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:57:52.768 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:57:52.768 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:57:55.394 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:57:55.396 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 58368
2025-02-22 20:57:55.397 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:57:55.437 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:57:55.438 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:57:55.459 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:57:55.482 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:57:55.493 - RealTimeSTT: root - ERROR - Transcription error: 'eng' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
2025-02-22 20:57:55.493 - RealTimeSTT: root - ERROR - Error during transcription: 'eng' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py", line 1464, in transcribe
    raise Exception(result)
Exception: 'eng' is not a valid language code (accepted language codes: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue)
2025-02-22 20:58:32.798 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-22 20:58:32.807 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-22 20:58:32.811 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-22 20:58:32.812 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-22 20:58:33.262 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-22 20:58:33.263 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-22 20:58:33.265 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-22 20:58:33.266 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-22 20:58:33.266 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-22 20:58:33.267 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-22 20:58:33.267 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-22 20:58:33.267 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-22 20:58:33.355 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-22 20:58:33.356 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-22 20:58:33.356 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-22 20:58:36.767 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-22 20:58:36.767 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-22 20:58:36.768 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:36.768 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:37.002 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:37.956 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:37.956 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:37.957 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:37.957 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:40.519 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:40.521 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 57344
2025-02-22 20:58:40.522 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:40.613 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:40.614 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:40.636 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:40.738 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:40.839 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:40.941 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:41.023 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:41.078 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.46 seconds
2025-02-22 20:58:41.078 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:41.078 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:41.078 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:43.849 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:43.850 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:43.850 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:43.850 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:45.704 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:45.706 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 46080
2025-02-22 20:58:45.706 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:45.734 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:45.735 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:45.736 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:45.838 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:45.938 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:46.040 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:46.141 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:46.177 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:46.199 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.46 seconds
2025-02-22 20:58:46.200 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:46.200 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:46.200 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:48.075 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:48.076 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:48.076 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:48.086 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:49.733 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:49.735 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 42496
2025-02-22 20:58:49.735 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:49.782 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:49.783 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:49.802 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:49.902 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:50.005 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:50.106 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:50.207 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:50.238 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:50.284 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.50 seconds
2025-02-22 20:58:50.285 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:50.285 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:50.285 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:51.722 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:51.722 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:51.722 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:51.723 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:53.381 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:53.383 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 43008
2025-02-22 20:58:53.383 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:53.455 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:53.456 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:53.464 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:53.569 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:53.677 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:53.778 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:53.879 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:53.908 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:53.937 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.48 seconds
2025-02-22 20:58:53.938 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:53.938 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:53.938 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:54.921 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:54.921 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:54.922 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:54.923 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:56.073 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:56.074 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-22 20:58:56.074 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:56.101 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:56.102 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:56.121 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:56.225 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:56.326 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:56.427 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:56.525 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:56.554 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.45 seconds
2025-02-22 20:58:56.554 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:56.555 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:56.555 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:56.708 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:56.708 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:56.708 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:56.709 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:58:57.989 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:58:57.990 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 36864
2025-02-22 20:58:57.990 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:58:58.045 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:58:58.045 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:58:58.056 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:58.157 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:58.259 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:58.360 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:58.461 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:58:58.463 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:58:58.497 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.45 seconds
2025-02-22 20:58:58.498 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:58:58.498 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:58:58.498 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:58:59.274 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:58:59.274 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:58:59.275 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:58:59.275 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:59:00.677 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:59:00.679 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 38912
2025-02-22 20:59:00.679 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:59:00.699 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:59:00.700 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:59:00.703 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:00.806 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:00.907 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:01.008 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:01.109 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:01.183 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:59:01.211 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.51 seconds
2025-02-22 20:59:01.211 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:59:01.211 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:59:01.211 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:59:02.279 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:59:02.279 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:59:02.279 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:59:02.280 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:59:04.708 - RealTimeSTT: root - INFO - recording stopped
2025-02-22 20:59:04.710 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 55296
2025-02-22 20:59:04.710 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-22 20:59:04.772 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-22 20:59:04.773 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-22 20:59:04.775 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:04.876 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:04.978 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:05.103 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:05.222 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:05.394 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:05.496 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-22 20:59:05.580 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-22 20:59:05.606 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.83 seconds
2025-02-22 20:59:05.607 - RealTimeSTT: root - INFO - Setting listen time
2025-02-22 20:59:05.607 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-22 20:59:05.607 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-22 20:59:07.146 - RealTimeSTT: root - INFO - voice activity detected
2025-02-22 20:59:07.146 - RealTimeSTT: root - INFO - recording started
2025-02-22 20:59:07.146 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-22 20:59:07.147 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-22 20:59:07.323 - RealTimeSTT: root - INFO - KeyboardInterrupt in wait_audio, shutting down
2025-02-22 20:59:07.324 - RealTimeSTT: root - DEBUG - Finishing recording thread
2025-02-22 20:59:07.328 - RealTimeSTT: root - DEBUG - Terminating reader process
2025-02-22 20:59:07.895 - RealTimeSTT: root - DEBUG - Terminating transcription process
2025-02-22 20:59:07.895 - RealTimeSTT: root - DEBUG - Finishing realtime thread
2025-02-22 20:59:07.970 - RealTimeSTT: root - INFO - KeyboardInterrupt in text() method
2025-02-23 00:33:32.939 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-23 00:33:32.950 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-23 00:33:32.955 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-23 00:33:32.955 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-23 00:33:33.433 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-23 00:33:33.434 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-23 00:33:33.436 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-23 00:33:33.437 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-23 00:33:33.437 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-23 00:33:33.438 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-23 00:33:33.438 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-23 00:33:33.438 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-23 00:33:33.537 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-23 00:33:33.538 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-23 00:33:33.538 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-23 00:33:37.268 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-23 00:33:37.268 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-23 00:33:37.268 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:33:37.268 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:33:37.527 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:33:39.298 - RealTimeSTT: root - INFO - voice activity detected
2025-02-23 00:33:39.298 - RealTimeSTT: root - INFO - recording started
2025-02-23 00:33:39.298 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-23 00:33:39.298 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-23 00:33:43.265 - RealTimeSTT: root - INFO - recording stopped
2025-02-23 00:33:43.271 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 79872
2025-02-23 00:33:43.271 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-23 00:33:43.327 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-23 00:33:43.328 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-23 00:33:43.350 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.464 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.565 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.667 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.770 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.872 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.972 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:43.978 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-23 00:33:44.000 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.67 seconds
2025-02-23 00:33:44.000 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:33:44.000 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:33:44.000 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:33:48.323 - RealTimeSTT: root - INFO - voice activity detected
2025-02-23 00:33:48.323 - RealTimeSTT: root - INFO - recording started
2025-02-23 00:33:48.324 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-23 00:33:48.324 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-23 00:33:49.475 - RealTimeSTT: root - INFO - recording stopped
2025-02-23 00:33:49.476 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 34816
2025-02-23 00:33:49.476 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-23 00:33:49.511 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-23 00:33:49.512 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-23 00:33:49.535 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:49.637 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:49.738 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:49.839 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:49.940 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:33:49.997 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-23 00:33:50.053 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.54 seconds
2025-02-23 00:33:50.053 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:33:50.053 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:33:50.053 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:34:15.145 - RealTimeSTT: root - INFO - KeyboardInterrupt in wait_audio, shutting down
2025-02-23 00:34:15.146 - RealTimeSTT: root - DEBUG - Receive from stdout pipe
2025-02-23 00:34:15.147 - RealTimeSTT: root - DEBUG - Finishing recording thread
2025-02-23 00:34:15.149 - RealTimeSTT: root - DEBUG - Terminating reader process
2025-02-23 00:34:15.714 - RealTimeSTT: root - DEBUG - Terminating transcription process
2025-02-23 00:34:15.714 - RealTimeSTT: root - DEBUG - Finishing realtime thread
2025-02-23 00:34:15.799 - RealTimeSTT: root - INFO - KeyboardInterrupt in text() method
2025-02-23 00:38:48.008 - RealTimeSTT: root - INFO - Starting RealTimeSTT
2025-02-23 00:38:48.017 - RealTimeSTT: root - INFO - Initializing audio recording (creating pyAudio input stream, sample rate: 16000 buffer size: 512
2025-02-23 00:38:48.020 - RealTimeSTT: root - INFO - Initializing WebRTC voice with Sensitivity 3
2025-02-23 00:38:48.020 - RealTimeSTT: root - DEBUG - WebRTC VAD voice activity detection engine initialized successfully
2025-02-23 00:38:48.495 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg6
2025-02-23 00:38:48.496 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg6 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so, 0x0006): Library not loaded: @rpath/libavutil.58.dylib
  Referenced from: <7DA1FD7D-D540-3C89-BD42-7A8ED4EC3111> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg6.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.58.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.58.dylib' (no such file)
2025-02-23 00:38:48.498 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg5
2025-02-23 00:38:48.499 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg5 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so, 0x0006): Library not loaded: @rpath/libavutil.57.dylib
  Referenced from: <DE9A5D63-A935-330C-9BA8-90240A45E34F> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg5.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.57.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.57.dylib' (no such file)
2025-02-23 00:38:48.499 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg4
2025-02-23 00:38:48.499 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg4 extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 108, in _find_versionsed_ffmpeg_extension
    _load_lib(lib)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 94, in _load_lib
    torch.ops.load_library(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torch/_ops.py", line 1357, in load_library
    ctypes.CDLL(path)
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/ctypes/__init__.py", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: dlopen(/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so, 0x0006): Library not loaded: @rpath/libavutil.56.dylib
  Referenced from: <C052F412-BF97-3CA9-A540-F5B67EF42B15> /opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/lib/libtorio_ffmpeg4.so
  Reason: tried: '/opt/anaconda3/envs/Testmate/lib/python3.12/lib-dynload/../../libavutil.56.dylib' (no such file), '/opt/anaconda3/envs/Testmate/bin/../lib/libavutil.56.dylib' (no such file)
2025-02-23 00:38:48.500 - RealTimeSTT: torio._extension.utils - DEBUG - Loading FFmpeg
2025-02-23 00:38:48.500 - RealTimeSTT: torio._extension.utils - DEBUG - Failed to load FFmpeg extension.
Traceback (most recent call last):
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 116, in _find_ffmpeg_extension
    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/torio/_extension/utils.py", line 106, in _find_versionsed_ffmpeg_extension
    raise RuntimeError(f"FFmpeg{version} extension is not available.")
RuntimeError: FFmpeg extension is not available.
2025-02-23 00:38:48.581 - RealTimeSTT: root - DEBUG - Silero VAD voice activity detection engine initialized successfully
2025-02-23 00:38:48.581 - RealTimeSTT: root - DEBUG - Starting realtime worker
2025-02-23 00:38:48.581 - RealTimeSTT: root - DEBUG - Waiting for main transcription model to start
2025-02-23 00:38:51.191 - RealTimeSTT: root - DEBUG - Main transcription model ready
2025-02-23 00:38:51.191 - RealTimeSTT: root - DEBUG - RealtimeSTT initialization completed successfully
2025-02-23 00:38:51.191 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:38:51.192 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:38:51.403 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:38:52.626 - RealTimeSTT: root - INFO - voice activity detected
2025-02-23 00:38:52.626 - RealTimeSTT: root - INFO - recording started
2025-02-23 00:38:52.627 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-23 00:38:52.627 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-23 00:38:57.039 - RealTimeSTT: root - INFO - recording stopped
2025-02-23 00:38:57.043 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 87040
2025-02-23 00:38:57.043 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-23 00:38:57.134 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-23 00:38:57.135 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-23 00:38:57.143 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.245 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.346 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.448 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.550 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.651 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.752 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.854 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.955 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:38:57.957 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-23 00:38:57.989 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.85 seconds
2025-02-23 00:38:59.514 - RealTimeSTT: ibm_watsonx_ai.client - INFO - Client successfully initialized
2025-02-23 00:39:02.586 - RealTimeSTT: ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'
2025-02-23 00:39:02.587 - RealTimeSTT: ibm_watsonx_ai.wml_resource - DEBUG - Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 200): {"total_count":27,"limit":200,"first":{"href":"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200"},"resources":[{"model_id":"codellama/codellama-34b-instruct-hf","label":"codellama-34b-instruct-hf","provider":"Code Llama","source":"Hugging Face","functions":[{"id":"text_generation"}],"short_description":"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.","long_description":"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.","input_tier":"class_2","output_tier":"class_2","number_params":"34b","min_shot_size":0,"task_ids":["code"],"tasks":[{"id":"code"}],"model_limits":{"max_sequence_length":16384,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-03-14"},{"id":"deprecated","start_date":"2025-01-15","alternative_model_ids":["llama-3-3-70b-instruct"]},{"id":"withdrawn","start_date":"2025-03-31","alternative_model_ids":["llama-3-3-70b-instruct"]}]},{"model_id":"google/flan-t5-xl","label":"flan-t5-xl-3b","provider":"Google","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.","long_description":"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.","input_tier":"class_1","output_tier":"class_1","number_params":"3b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization","tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation"},{"id":"classification","tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"extraction"}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-12-07"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":256},"max_output_tokens":{"default":128,"min":1,"max":128},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.3,"min":0.00001,"max":0.5}}},{"model_id":"google/flan-t5-xxl","label":"flan-t5-xxl-11b","provider":"Google","source":"Hugging Face","functions":[{"id":"text_generation"}],"short_description":"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.","long_description":"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.","input_tier":"class_2","output_tier":"class_2","number_params":"11b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":4}},{"id":"retrieval_augmented_generation","ratings":{"quality":3}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-07-07"}]},{"model_id":"google/flan-ul2","label":"flan-ul2-20b","provider":"Google","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"text_generation"}],"short_description":"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.","long_description":"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.","input_tier":"class_3","output_tier":"class_3","number_params":"20b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":0}],"tags":["Reasoning"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":23.71}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":19.81}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":5.19}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":49.23}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":35.87}],"tags":["Reasoning"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":57.13}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":70.71}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":62}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":58.23}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":28.57}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":0},"metrics":[{"name":"SacreBLEU","value":15.89}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":0}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":89.2}],"tags":["Safety & Bias"]}]},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":20.5}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":6.35}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":22.45}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-07-07"}]},{"model_id":"ibm/granite-13b-instruct-v2","label":"granite-13b-instruct-v2","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":3},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":0}],"tags":["Reasoning"]}]},{"id":"summarization","ratings":{"quality":2},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Please write a summary highlighting the main points of the following text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":40,"min":1,"max":50},"verbalizer":{"default":"Please write a summary highlighting the main points of the following text: {{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":1,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":22.68}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":32.64}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.11}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.19}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":13.95}],"tags":["Reasoning"]}]},{"id":"classification","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Classify the text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":32,"min":1,"max":128},"learning_rate":{"default":0.0006,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":46.72}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":0}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":31.27}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":3.85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":20.44}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":16.33}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53}],"tags":["Reasoning"]}]},{"id":"generation","tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":13.21}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":2.42}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":43.71}],"tags":["Safety & Bias"]}]},{"id":"extraction","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.26}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":0}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":12.34}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8191,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8191},"v2-professional":{"call_time":"10m0s","max_output_tokens":8191},"v2-standard":{"call_time":"10m0s","max_output_tokens":8191}},"lifecycle":[{"id":"available","start_date":"2023-12-01"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}}},{"model_id":"ibm/granite-20b-code-instruct","label":"granite-20b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"20b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-05-06"}],"versions":[{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-06"}]},{"model_id":"ibm/granite-20b-multilingual","label":"granite-20b-multilingual","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"20b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":3},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":10}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":89.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.24}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":85.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":85.72}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":25.09}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":29.67}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.02}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":26.61}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.93}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":22.45}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.75}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":35.45}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":46.2}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.42}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":80.71}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.74}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":74.95}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":49.35}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":36.78}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.36}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":59.65}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":34}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":40.29}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":22.45}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.55}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.03}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.84}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.38}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.65}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.07}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.48}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.95}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":10.94}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":1.56}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":91.19}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.89}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":27.45}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.88}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.46}],"tags":["Generation"]}]},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":32.35}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":33.71}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":27.36}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":89.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":86.24}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":88.94}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-03-14"},{"id":"deprecated","start_date":"2025-01-15","alternative_model_ids":["ibm/granite-3-8b-instruct"]},{"id":"withdrawn","start_date":"2025-04-16","alternative_model_ids":["ibm/granite-3-8b-instruct"]}],"versions":[{"version":"1.1.0","available_date":"2024-04-18"},{"version":"1.0.0","available_date":"2024-03-14"}],"supported_languages":["en","de","es","fr","pt"]},{"model_id":"ibm/granite-3-2-8b-instruct-preview-rc","label":"granite-3-2-8b-instruct-preview-rc","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"","long_description":"","input_tier":"tech_preview","output_tier":"tech_preview","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling","ratings":{"quality":3}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-02-07"}],"versions":[{"version":"1.0.0","available_date":"2024-02-07"}]},{"model_id":"ibm/granite-3-2b-instruct","label":"granite-3-2b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":80.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.56}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":40.41}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":82.69}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.8}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.06}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.66}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.21}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.9}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.77}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.21}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.12}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":48.37}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.85}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.13}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.52}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.82}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.88}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.95}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.28}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.18}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.41}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.52}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.81}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.98}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.69}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":37.24}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.77}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction"},{"id":"translation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":76.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":69.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":74.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":80.71}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-3-8b-instruct","label":"granite-3-8b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.95}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":50.14}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":83.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.14}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.39}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.48}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.42}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.5}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.05}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.6}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":81.69}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.54}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.23}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.15}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56.28}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.27}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":48.68}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":46.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":40.92}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":48.74}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.25}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.7}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.11}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":23.96}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.88}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.29}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.35}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction"},{"id":"translation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":87.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":85.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":75.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":82.12}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":87.53}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":3}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-34b-code-instruct","label":"granite-34b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"34b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-05-06"}]},{"model_id":"ibm/granite-3b-code-instruct","label":"granite-3b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"3b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-05-09"}],"versions":[{"version":"1.1.1","available_date":"2024-10-28"},{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-09"}]},{"model_id":"ibm/granite-8b-code-instruct","label":"granite-8b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-05-09"}],"versions":[{"version":"1.1.1","available_date":"2024-10-28"},{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-09"}]},{"model_id":"ibm/granite-guardian-3-2b","label":"granite-guardian-3-2b","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2025-01-15"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-guardian-3-8b","label":"granite-guardian-3-8b","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2025-01-15"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"meta-llama/llama-2-13b-chat","label":"llama-2-13b-chat","provider":"Meta","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4},"tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-11-09"},{"id":"deprecated","start_date":"2024-08-26"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.002,"min":0.00001,"max":0.5}}},{"model_id":"meta-llama/llama-3-1-70b-instruct","label":"llama-3-1-70b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-1-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-1-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_2","output_tier":"class_2","number_params":"70b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":33}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":61.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.72}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":31.51}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":56.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.35}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":78.25}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.08}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":24.15}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.45}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.09}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":6.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":21.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":11.68}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":20.06}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.75}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":56.74}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":46.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.75}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.45}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.84}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.48}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":59.59}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":74.61}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.73}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":66.26}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56.12}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.97}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.19}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.65}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.87}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.62}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.11}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.07}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.26}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.69}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.62}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.71}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.38}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.71}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.17}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.56}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.86}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.54}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.7}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.73}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.05}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.19}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":9.26}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":23.7}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":77.55}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.33}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":28.45}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.73}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.2}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":51.17}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":38.98}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":41.8}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":81.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":51.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.18}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":92}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-08-07"},{"id":"deprecated","start_date":"2025-01-22","alternative_model_ids":["llama-3-3-70b-instruct","llama-3-2-90b-vision-instruct"]},{"id":"withdrawn","start_date":"2025-05-30","alternative_model_ids":["llama-3-3-70b-instruct","llama-3-2-90b-vision-instruct"]}],"versions":[{"version":"3.1.0","available_date":"2024-08-07"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-1-8b-instruct","label":"llama-3-1-8b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-1-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-1-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_1","output_tier":"class_1","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":15}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":55.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":76.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":78.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":44.5}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":48.34}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":77.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":70.08}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":31.84}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":21.06}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.37}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.33}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.91}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.63}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.71}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":8.81}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.73}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.96}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":45.78}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":39.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.95}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.09}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.85}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":52.56}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":1.89}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.82}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":0}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":8}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.82}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.88}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.3}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.35}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.27}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.47}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.33}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.42}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":47.05}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":44.87}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.42}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.09}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.73}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.52}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.23}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.63}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.89}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":8.94}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":3.4}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":73.21}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":20.73}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":21.36}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":34.32}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.35}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":46.79}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":11.29}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":32.77}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":89.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":77.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":47.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.82}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-08-01"},{"id":"deprecated","start_date":"2025-01-22","alternative_model_ids":["llama-3-2-11b-vision-instruct"]},{"id":"withdrawn","start_date":"2025-05-30","alternative_model_ids":["llama-3-2-11b-vision-instruct"]}],"versions":[{"version":"3.1.0","available_date":"2024-08-01"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-2-11b-vision-instruct","label":"llama-3-2-11b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-11b-vision-instruc is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-11b-vision-instruc is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-1b-instruct","label":"llama-3-2-1b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-1b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-1b-instruct is a pretrained and fine-tuned generative text model with 1 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_c1","output_tier":"class_c1","number_params":"1b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-3b-instruct","label":"llama-3-2-3b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-3b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-3b-instruct is a pretrained and fine-tuned generative text model with 3 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_8","output_tier":"class_8","number_params":"3b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-90b-vision-instruct","label":"llama-3-2-90b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-90b-vision-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-90b-vision-instruct is a pretrained and fine-tuned generative text model with 90 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_10","output_tier":"class_10","number_params":"90b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-3-70b-instruct","label":"llama-3-3-70b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"This version of Llama-3.3-70b-instruct is also the FP8 quantized version of the original FP16 weights.","long_description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.","input_tier":"class_13","output_tier":"class_13","number_params":"70b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-12-06"}],"versions":[{"version":"3.3.0","available_date":"2024-12-06"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-405b-instruct","label":"llama-3-405b-instruct","provider":"Meta","source":"Meta","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases","long_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases. It's also the largest open-sourced model ever released. It can also be used as a synthetic data generator, post-training data ranking judge, or model teacher/supervisor that can improve specialized capabilities in derivative, more inference friendly models.","input_tier":"class_3","output_tier":"class_7","number_params":"405b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-07-23"}],"versions":[{"version":"3.1.0","available_date":"2024-07-23"}]},{"model_id":"meta-llama/llama-guard-3-11b-vision","label":"llama-guard-3-11b-vision","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-guard-3-11b-vision is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-guard-3-11b-vision is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"mistralai/mistral-large","label":"mistral-large","provider":"Mistral AI","source":"Mistral","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Mistral Large, the most advanced Large Language Model (LLM) developed by Mistral Al, is an exceptionally powerful model. Thanks to its state-of-the-art reasoning capabilities it can be applied to any language-based task, including the most sophisticated ones.","long_description":"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized. For comprehensive information and examples about this model, please refer to the release blog post.","input_tier":"mistral_large_input","output_tier":"mistral_large","number_params":"","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":35}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":64.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.97}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":89.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":53.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":51.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.21}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.16}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.84}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":26.75}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.47}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.99}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.28}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.7}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":10.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.21}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":60.5}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":50.57}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.47}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.12}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.43}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.14}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":63.6}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.44}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":61.28}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.41}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":97.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.9}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.94}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.98}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.33}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":67.73}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.53}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.25}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.46}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.39}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.97}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":12.1}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":44.08}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":65.34}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":27.62}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":30.38}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.19}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.06}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":51.8}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.7}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":40.25}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":93.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":83.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":53.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.59}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.47}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-07-09"}],"versions":[{"version":"2.0.0","available_date":"2024-07-24"},{"version":"1.0.0","available_date":"2024-07-09"}],"supported_languages":["en","fr","de","it","zh","ja","ko","pt","nl","pl"]},{"model_id":"mistralai/mixtral-8x7b-instruct-v01","label":"mixtral-8x7b-instruct-v01","provider":"Mistral AI","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.","long_description":"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.","input_tier":"class_1","output_tier":"class_1","number_params":"46.7b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":11}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":52.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.26}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":88.76}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":45.23}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":50.19}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.96}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.45}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":27.13}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":30.76}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.51}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.44}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.89}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.69}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":12.85}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":8.65}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.18}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.01}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":39.49}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":50.48}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.18}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.26}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.48}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.05}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":59.05}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":47.2}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.82}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":65.89}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":54.91}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.78}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.34}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.59}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.54}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.39}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.74}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.15}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.29}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.82}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.99}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":44.16}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":57.55}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":56.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.33}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.54}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":57.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":58.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.45}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.78}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":9.98}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":14.75}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":69.44}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.34}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.49}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.34}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.51}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":42.74}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.4}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":37.27}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":90.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":76.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":45.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":90}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":91.65}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":32768,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-04-17"}],"supported_languages":["en","fr","de","it","es"]}]}
2025-02-23 00:39:02.593 - RealTimeSTT: httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-02-23 00:39:02.594 - RealTimeSTT: httpx - DEBUG - load_verify_locations cafile='/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/certifi/cacert.pem'
2025-02-23 00:39:02.606 - RealTimeSTT: httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-02-23 00:39:02.607 - RealTimeSTT: httpx - DEBUG - load_verify_locations cafile='/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/certifi/cacert.pem'
2025-02-23 00:39:02.616 - RealTimeSTT: asyncio - DEBUG - Using selector: KqueueSelector
2025-02-23 00:39:02.623 - RealTimeSTT: httpcore.connection - DEBUG - connect_tcp.started host='us-south.ml.cloud.ibm.com' port=443 local_address=None timeout=10 socket_options=None
2025-02-23 00:39:02.884 - RealTimeSTT: httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x30e4e8920>
2025-02-23 00:39:02.884 - RealTimeSTT: httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x30ef7a350> server_hostname='us-south.ml.cloud.ibm.com' timeout=10
2025-02-23 00:39:02.902 - RealTimeSTT: httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x30ec98c20>
2025-02-23 00:39:02.902 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-23 00:39:02.903 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-23 00:39:02.903 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-23 00:39:02.904 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_body.complete
2025-02-23 00:39:02.904 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-23 00:39:04.473 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 22 Feb 2025 17:39:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'server-timing', b'intid;desc=fc87a044ed1150b3'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'Pragma', b'no-cache'), (b'Cache-Control', b'no-cache, no-store, must-revalidate'), (b'X-Requests-Limit-Period', b'1000'), (b'X-Requests-Limit-Reset-After', b'125'), (b'Content-Security-Policy', b"default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self'; frame-ancestors 'none'; form-action 'self';"), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Frame-Options', b'DENY'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Requests-Limit-Rate', b'8'), (b'X-Requests-Limit-Remaining', b'7'), (b'X-Global-Transaction-Id', b'81a46fd17bc8386fa0ed56ee47fb25b4'), (b'Referrer-Policy', b'strict-origin'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9160c0d74b93d339-BKK'), (b'Content-Encoding', b'gzip')])
2025-02-23 00:39:04.475 - RealTimeSTT: httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12 "HTTP/1.1 200 OK"
2025-02-23 00:39:04.475 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-23 00:39:04.475 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-23 00:39:04.476 - RealTimeSTT: httpcore.http11 - DEBUG - response_closed.started
2025-02-23 00:39:04.476 - RealTimeSTT: httpcore.http11 - DEBUG - response_closed.complete
2025-02-23 00:39:04.476 - RealTimeSTT: ibm_watsonx_ai.wml_resource - INFO - Successfully finished achat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12'
2025-02-23 00:39:04.476 - RealTimeSTT: ibm_watsonx_ai.wml_resource - DEBUG - Response(POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12 200): {"id": "chatcmpl-81a46fd17bc8386fa0ed56ee47fb25b4", "model_id": "ibm/granite-3-8b-instruct", "model": "ibm/granite-3-8b-instruct", "choices": "...", "created": 1740245943, "model_version": "1.1.0", "created_at": "2025-02-22T17:39:04.389Z", "usage": {"completion_tokens": 70, "prompt_tokens": 40, "total_tokens": 110}, "system": {"warnings": [{"message": "The value of 'max_tokens' for this model was set to value 1024", "id": "unspecified_max_token", "additional_properties": {"limit": 0, "new_value": 1024, "parameter": "max_tokens", "value": 0}}]}}
2025-02-23 00:39:04.478 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:39:04.478 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:39:04.478 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:39:18.607 - RealTimeSTT: root - INFO - voice activity detected
2025-02-23 00:39:18.609 - RealTimeSTT: root - INFO - recording started
2025-02-23 00:39:18.609 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-23 00:39:18.609 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-23 00:39:22.512 - RealTimeSTT: root - INFO - recording stopped
2025-02-23 00:39:22.518 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 78848
2025-02-23 00:39:22.518 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-23 00:39:22.623 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-23 00:39:22.624 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-23 00:39:22.640 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:22.742 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:22.843 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:22.944 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:23.046 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:23.107 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-23 00:39:23.143 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.52 seconds
2025-02-23 00:39:24.375 - RealTimeSTT: ibm_watsonx_ai.client - INFO - Client successfully initialized
2025-02-23 00:39:25.840 - RealTimeSTT: ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'
2025-02-23 00:39:25.841 - RealTimeSTT: ibm_watsonx_ai.wml_resource - DEBUG - Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 200): {"total_count":27,"limit":200,"first":{"href":"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-02-12&project_id=1204bd52-d88c-498b-b06b-fdf73acba30d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200"},"resources":[{"model_id":"codellama/codellama-34b-instruct-hf","label":"codellama-34b-instruct-hf","provider":"Code Llama","source":"Hugging Face","functions":[{"id":"text_generation"}],"short_description":"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.","long_description":"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.","input_tier":"class_2","output_tier":"class_2","number_params":"34b","min_shot_size":0,"task_ids":["code"],"tasks":[{"id":"code"}],"model_limits":{"max_sequence_length":16384,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-03-14"},{"id":"deprecated","start_date":"2025-01-15","alternative_model_ids":["llama-3-3-70b-instruct"]},{"id":"withdrawn","start_date":"2025-03-31","alternative_model_ids":["llama-3-3-70b-instruct"]}]},{"model_id":"google/flan-t5-xl","label":"flan-t5-xl-3b","provider":"Google","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.","long_description":"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.","input_tier":"class_1","output_tier":"class_1","number_params":"3b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization","tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation"},{"id":"classification","tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"extraction"}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-12-07"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":256},"max_output_tokens":{"default":128,"min":1,"max":128},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.3,"min":0.00001,"max":0.5}}},{"model_id":"google/flan-t5-xxl","label":"flan-t5-xxl-11b","provider":"Google","source":"Hugging Face","functions":[{"id":"text_generation"}],"short_description":"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.","long_description":"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.","input_tier":"class_2","output_tier":"class_2","number_params":"11b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":4}},{"id":"retrieval_augmented_generation","ratings":{"quality":3}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-07-07"}]},{"model_id":"google/flan-ul2","label":"flan-ul2-20b","provider":"Google","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"text_generation"}],"short_description":"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.","long_description":"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.","input_tier":"class_3","output_tier":"class_3","number_params":"20b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":0}],"tags":["Reasoning"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":23.71}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":19.81}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":5.19}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":49.23}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":35.87}],"tags":["Reasoning"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":57.13}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":70.71}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":62}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":58.23}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":28.57}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":0},"metrics":[{"name":"SacreBLEU","value":15.89}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":0}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":89.2}],"tags":["Safety & Bias"]}]},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":20.5}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":6.35}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":22.45}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-07-07"}]},{"model_id":"ibm/granite-13b-instruct-v2","label":"granite-13b-instruct-v2","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":3},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":0}],"tags":["Reasoning"]}]},{"id":"summarization","ratings":{"quality":2},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Please write a summary highlighting the main points of the following text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":40,"min":1,"max":50},"verbalizer":{"default":"Please write a summary highlighting the main points of the following text: {{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":1,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":22.68}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":32.64}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.11}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.19}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":13.95}],"tags":["Reasoning"]}]},{"id":"classification","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Classify the text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":32,"min":1,"max":128},"learning_rate":{"default":0.0006,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":46.72}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":0}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":31.27}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":3.85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":20.44}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":16.33}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53}],"tags":["Reasoning"]}]},{"id":"generation","tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":13.21}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":2.42}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":43.71}],"tags":["Safety & Bias"]}]},{"id":"extraction","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.26}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":0}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":12.34}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8191,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8191},"v2-professional":{"call_time":"10m0s","max_output_tokens":8191},"v2-standard":{"call_time":"10m0s","max_output_tokens":8191}},"lifecycle":[{"id":"available","start_date":"2023-12-01"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}}},{"model_id":"ibm/granite-20b-code-instruct","label":"granite-20b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"20b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-05-06"}],"versions":[{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-06"}]},{"model_id":"ibm/granite-20b-multilingual","label":"granite-20b-multilingual","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"20b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":3},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":10}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":89.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.24}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":85.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":85.72}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":25.09}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":29.67}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.02}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":26.61}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.93}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":22.45}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.75}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":35.45}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":46.2}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.42}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":80.71}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.74}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":74.95}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":49.35}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":36.78}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.36}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":59.65}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":34}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":40.29}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":22.45}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.55}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.03}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.84}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.38}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.65}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.07}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.48}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.95}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":10.94}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":1.56}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":91.19}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.89}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":27.45}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.88}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.46}],"tags":["Generation"]}]},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":32.35}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":33.71}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":27.36}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":89.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":86.24}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":88.94}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-03-14"},{"id":"deprecated","start_date":"2025-01-15","alternative_model_ids":["ibm/granite-3-8b-instruct"]},{"id":"withdrawn","start_date":"2025-04-16","alternative_model_ids":["ibm/granite-3-8b-instruct"]}],"versions":[{"version":"1.1.0","available_date":"2024-04-18"},{"version":"1.0.0","available_date":"2024-03-14"}],"supported_languages":["en","de","es","fr","pt"]},{"model_id":"ibm/granite-3-2-8b-instruct-preview-rc","label":"granite-3-2-8b-instruct-preview-rc","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"","long_description":"","input_tier":"tech_preview","output_tier":"tech_preview","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling","ratings":{"quality":3}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-02-07"}],"versions":[{"version":"1.0.0","available_date":"2024-02-07"}]},{"model_id":"ibm/granite-3-2b-instruct","label":"granite-3-2b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":80.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.56}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":40.41}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":82.69}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.8}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.06}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.66}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.21}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.9}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.77}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.21}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.12}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":48.37}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.85}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.13}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.52}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.82}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.88}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.95}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.28}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.18}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.41}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.52}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.81}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.98}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":24.69}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":37.24}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.77}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction"},{"id":"translation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":76.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":69.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":74.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":80.71}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-3-8b-instruct","label":"granite-3-8b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.95}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":50.14}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":83.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.14}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.39}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.48}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.42}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.5}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.05}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.6}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":81.69}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.54}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.23}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.15}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56.28}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.27}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":48.68}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":46.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":40.92}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":48.74}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.25}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.7}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.11}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":23.96}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.88}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.29}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.35}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction"},{"id":"translation","benchmarks":[{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":87.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":85.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":75.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":82.12}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":87.53}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":3}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-34b-code-instruct","label":"granite-34b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"34b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-05-06"}]},{"model_id":"ibm/granite-3b-code-instruct","label":"granite-3b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"3b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-05-09"}],"versions":[{"version":"1.1.1","available_date":"2024-10-28"},{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-09"}]},{"model_id":"ibm/granite-8b-code-instruct","label":"granite-8b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_1","output_tier":"class_1","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-05-09"}],"versions":[{"version":"1.1.1","available_date":"2024-10-28"},{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-09"}]},{"model_id":"ibm/granite-guardian-3-2b","label":"granite-guardian-3-2b","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2025-01-15"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-guardian-3-8b","label":"granite-guardian-3-8b","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2025-01-15"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"meta-llama/llama-2-13b-chat","label":"llama-2-13b-chat","provider":"Meta","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4},"tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4095},"v2-professional":{"call_time":"10m0s","max_output_tokens":4095},"v2-standard":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-11-09"},{"id":"deprecated","start_date":"2024-08-26"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.002,"min":0.00001,"max":0.5}}},{"model_id":"meta-llama/llama-3-1-70b-instruct","label":"llama-3-1-70b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-1-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-1-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_2","output_tier":"class_2","number_params":"70b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":33}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":61.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":81.72}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":31.51}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":56.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.35}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":78.25}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.08}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":24.15}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.45}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":18.09}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":6.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":21.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":19.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":11.68}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":20.06}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.75}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":56.74}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":46.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.75}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.45}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.84}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.48}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":59.59}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":74.61}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.73}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":66.26}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":56.12}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.97}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.19}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.65}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.87}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.62}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.11}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.07}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.26}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.69}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.62}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.71}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.38}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.71}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.17}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.56}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.86}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.54}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.82}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.7}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.73}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.05}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.19}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":9.26}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":23.7}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":77.55}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.33}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":28.45}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.73}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.2}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":51.17}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":38.98}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":41.8}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":81.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":51.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.18}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":92}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-08-07"},{"id":"deprecated","start_date":"2025-01-22","alternative_model_ids":["llama-3-3-70b-instruct","llama-3-2-90b-vision-instruct"]},{"id":"withdrawn","start_date":"2025-05-30","alternative_model_ids":["llama-3-3-70b-instruct","llama-3-2-90b-vision-instruct"]}],"versions":[{"version":"3.1.0","available_date":"2024-08-07"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-1-8b-instruct","label":"llama-3-1-8b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-1-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-1-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_1","output_tier":"class_1","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":15}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":55.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":76.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":78.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":44.5}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":48.34}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":77.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":70.08}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":31.84}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":21.06}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.37}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.33}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.91}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.63}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.71}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":8.81}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.73}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.96}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":45.78}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":39.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.95}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.09}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.85}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":52.56}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":1.89}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.82}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":0}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":8}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.82}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.88}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.3}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.35}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.27}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.47}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.33}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.42}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":47.05}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":44.87}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.42}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.09}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.73}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.52}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.23}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.63}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.89}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":8.94}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":3.4}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":73.21}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":20.73}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":21.36}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":34.32}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.35}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":46.79}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":11.29}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":32.77}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":89.76}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":77.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":47.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.82}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-08-01"},{"id":"deprecated","start_date":"2025-01-22","alternative_model_ids":["llama-3-2-11b-vision-instruct"]},{"id":"withdrawn","start_date":"2025-05-30","alternative_model_ids":["llama-3-2-11b-vision-instruct"]}],"versions":[{"version":"3.1.0","available_date":"2024-08-01"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-2-11b-vision-instruct","label":"llama-3-2-11b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-11b-vision-instruc is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-11b-vision-instruc is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-1b-instruct","label":"llama-3-2-1b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-1b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-1b-instruct is a pretrained and fine-tuned generative text model with 1 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_c1","output_tier":"class_c1","number_params":"1b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-3b-instruct","label":"llama-3-2-3b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-3b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-3b-instruct is a pretrained and fine-tuned generative text model with 3 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_8","output_tier":"class_8","number_params":"3b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-90b-vision-instruct","label":"llama-3-2-90b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-90b-vision-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-90b-vision-instruct is a pretrained and fine-tuned generative text model with 90 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_10","output_tier":"class_10","number_params":"90b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-3-70b-instruct","label":"llama-3-3-70b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"This version of Llama-3.3-70b-instruct is also the FP8 quantized version of the original FP16 weights.","long_description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.","input_tier":"class_13","output_tier":"class_13","number_params":"70b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-12-06"}],"versions":[{"version":"3.3.0","available_date":"2024-12-06"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-405b-instruct","label":"llama-3-405b-instruct","provider":"Meta","source":"Meta","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases","long_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases. It's also the largest open-sourced model ever released. It can also be used as a synthetic data generator, post-training data ranking judge, or model teacher/supervisor that can improve specialized capabilities in derivative, more inference friendly models.","input_tier":"class_3","output_tier":"class_7","number_params":"405b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":4096},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":4096},"v2-professional":{"call_time":"10m0s","max_output_tokens":4096},"v2-standard":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-07-23"}],"versions":[{"version":"3.1.0","available_date":"2024-07-23"}]},{"model_id":"meta-llama/llama-guard-3-11b-vision","label":"llama-guard-3-11b-vision","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-guard-3-11b-vision is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-guard-3-11b-vision is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":8192},"v2-professional":{"call_time":"10m0s","max_output_tokens":8192},"v2-standard":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"mistralai/mistral-large","label":"mistral-large","provider":"Mistral AI","source":"Mistral","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Mistral Large, the most advanced Large Language Model (LLM) developed by Mistral Al, is an exceptionally powerful model. Thanks to its state-of-the-art reasoning capabilities it can be applied to any language-based task, including the most sophisticated ones.","long_description":"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized. For comprehensive information and examples about this model, please refer to the release blog post.","input_tier":"mistral_large_input","output_tier":"mistral_large","number_params":"","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":35}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":64.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.97}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":89.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":53.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":51.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.21}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.16}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.84}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":26.75}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.47}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.99}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":17.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.28}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":15.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.7}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":10.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":13.21}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":60.5}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":50.57}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.47}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.12}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.43}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.14}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":63.6}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.44}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":61.28}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.41}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":97.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.9}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.94}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.98}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.33}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":67.73}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.53}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.25}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.63}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.46}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.39}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.97}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":12.1}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":44.08}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":65.34}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":27.62}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":30.38}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.19}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.06}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":51.8}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.7}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":40.25}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":93.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":83.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":53.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.59}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.47}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-07-09"}],"versions":[{"version":"2.0.0","available_date":"2024-07-24"},{"version":"1.0.0","available_date":"2024-07-09"}],"supported_languages":["en","fr","de","it","zh","ja","ko","pt","nl","pl"]},{"model_id":"mistralai/mixtral-8x7b-instruct-v01","label":"mixtral-8x7b-instruct-v01","provider":"Mistral AI","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.","long_description":"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.","input_tier":"class_1","output_tier":"class_1","number_params":"46.7b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":11}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":52.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":87.26}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":88.76}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":45.23}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":50.19}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":84.96}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":86.45}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":27.13}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":30.76}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.51}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.44}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":3.89}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":16.69}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":12.85}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":8.65}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.18}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":14.01}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":39.49}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":50.48}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.18}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.26}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.48}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to retrieve and rank multilingual text. Metric shows the Normalized Discounted Cumulative Gain (NDCG) score for the information retrieval and ranking.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.05}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":59.05}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":47.2}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.82}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":65.89}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":54.91}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.78}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.34}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":87.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.59}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.54}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.39}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.74}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":81.15}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.29}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.82}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.99}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english_choices"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":44.16}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":57.55}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":56.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.33}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.54}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":57.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":58.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.45}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.78}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":9.98}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":14.75}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":69.44}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.34}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.49}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.34}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of 11 tasks that span 19 languages to evaluate a model's ability to understand multilingual text and generate insightful questions about it. Metric shows the ROUGE-L score for the generated question.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.51}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":42.74}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.4}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLORES-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":37.27}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":92.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":90.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"Accuracy","value":76.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":45.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":90}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"Accuracy","value":91.65}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":32768,"max_output_tokens":16384},"limits":{"lite":{"call_time":"5m0s","max_output_tokens":16384},"v2-professional":{"call_time":"10m0s","max_output_tokens":16384},"v2-standard":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-04-17"}],"supported_languages":["en","fr","de","it","es"]}]}
2025-02-23 00:39:25.848 - RealTimeSTT: httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-02-23 00:39:25.849 - RealTimeSTT: httpx - DEBUG - load_verify_locations cafile='/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/certifi/cacert.pem'
2025-02-23 00:39:25.862 - RealTimeSTT: httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2025-02-23 00:39:25.862 - RealTimeSTT: httpx - DEBUG - load_verify_locations cafile='/opt/anaconda3/envs/Testmate/lib/python3.12/site-packages/certifi/cacert.pem'
2025-02-23 00:39:25.872 - RealTimeSTT: asyncio - DEBUG - Using selector: KqueueSelector
2025-02-23 00:39:25.873 - RealTimeSTT: httpcore.connection - DEBUG - connect_tcp.started host='us-south.ml.cloud.ibm.com' port=443 local_address=None timeout=10 socket_options=None
2025-02-23 00:39:26.131 - RealTimeSTT: httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x30eeb30e0>
2025-02-23 00:39:26.132 - RealTimeSTT: httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x30efdf1d0> server_hostname='us-south.ml.cloud.ibm.com' timeout=10
2025-02-23 00:39:26.144 - RealTimeSTT: httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x30ee46f30>
2025-02-23 00:39:26.144 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-23 00:39:26.145 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-23 00:39:26.145 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-23 00:39:26.145 - RealTimeSTT: httpcore.http11 - DEBUG - send_request_body.complete
2025-02-23 00:39:26.145 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-23 00:39:27.959 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 22 Feb 2025 17:39:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'server-timing', b'intid;desc=2b005758251ad40e'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'X-Frame-Options', b'DENY'), (b'Referrer-Policy', b'strict-origin'), (b'X-Global-Transaction-Id', b'c4685b879c44662a133a6f01a824601a'), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Requests-Limit-Remaining', b'7'), (b'X-Requests-Limit-Reset-After', b'125'), (b'X-Requests-Limit-Rate', b'8'), (b'X-Content-Type-Options', b'nosniff'), (b'Cache-Control', b'no-cache, no-store, must-revalidate'), (b'X-Requests-Limit-Period', b'1000'), (b'Content-Security-Policy', b"default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self'; frame-ancestors 'none'; form-action 'self';"), (b'Pragma', b'no-cache'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9160c1688de6d33c-BKK'), (b'Content-Encoding', b'gzip')])
2025-02-23 00:39:27.959 - RealTimeSTT: httpx - INFO - HTTP Request: POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12 "HTTP/1.1 200 OK"
2025-02-23 00:39:27.959 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-23 00:39:27.960 - RealTimeSTT: httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-23 00:39:27.960 - RealTimeSTT: httpcore.http11 - DEBUG - response_closed.started
2025-02-23 00:39:27.960 - RealTimeSTT: httpcore.http11 - DEBUG - response_closed.complete
2025-02-23 00:39:27.960 - RealTimeSTT: ibm_watsonx_ai.wml_resource - INFO - Successfully finished achat for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12'
2025-02-23 00:39:27.960 - RealTimeSTT: ibm_watsonx_ai.wml_resource - DEBUG - Response(POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-02-12 200): {"id": "chatcmpl-c4685b879c44662a133a6f01a824601a", "model_id": "ibm/granite-3-8b-instruct", "model": "ibm/granite-3-8b-instruct", "choices": "...", "created": 1740245967, "model_version": "1.1.0", "created_at": "2025-02-22T17:39:27.883Z", "usage": {"completion_tokens": 52, "prompt_tokens": 32, "total_tokens": 84}, "system": {"warnings": [{"message": "The value of 'max_tokens' for this model was set to value 1024", "id": "unspecified_max_token", "additional_properties": {"limit": 0, "new_value": 1024, "parameter": "max_tokens", "value": 0}}]}}
2025-02-23 00:39:27.961 - RealTimeSTT: root - INFO - Setting listen time
2025-02-23 00:39:27.961 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'listening'
2025-02-23 00:39:27.961 - RealTimeSTT: root - DEBUG - Waiting for recording start
2025-02-23 00:39:29.811 - RealTimeSTT: root - INFO - voice activity detected
2025-02-23 00:39:29.811 - RealTimeSTT: root - INFO - recording started
2025-02-23 00:39:29.812 - RealTimeSTT: root - INFO - State changed from 'listening' to 'recording'
2025-02-23 00:39:29.812 - RealTimeSTT: root - DEBUG - Waiting for recording stop
2025-02-23 00:39:33.519 - RealTimeSTT: root - INFO - recording stopped
2025-02-23 00:39:33.521 - RealTimeSTT: root - DEBUG - No samples removed, final audio length: 75776
2025-02-23 00:39:33.521 - RealTimeSTT: root - INFO - State changed from 'recording' to 'inactive'
2025-02-23 00:39:33.526 - RealTimeSTT: root - INFO - State changed from 'inactive' to 'transcribing'
2025-02-23 00:39:33.527 - RealTimeSTT: root - DEBUG - Adding transcription request, no early transcription started
2025-02-23 00:39:33.547 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:33.649 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:33.750 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:33.852 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:33.953 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:34.054 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:34.156 - RealTimeSTT: root - DEBUG - Receive from parent_transcription_pipe after sendiung transcription request, transcribe_count: 1
2025-02-23 00:39:34.209 - RealTimeSTT: root - INFO - State changed from 'transcribing' to 'inactive'
2025-02-23 00:39:34.259 - RealTimeSTT: root - DEBUG - Model tiny completed transcription in 0.73 seconds
2025-02-23 00:39:35.189 - RealTimeSTT: ibm_watsonx_ai.client - INFO - Client successfully initialized
2025-02-23 00:39:36.281 - RealTimeSTT: root - DEBUG - Receive from stdout pipe
